{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get some data\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def maybe_download(data_path='reuters21578.tar.gz'):\n",
    "    \"\"\"might download Reuters-21578\"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print('Downloading dataset :)')\n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz'\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(data_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk: # could be keep-alive chunks\n",
    "                    f.write(chunk)\n",
    "    # quickly validate the data\n",
    "    size = os.path.getsize(data_path) >> 20\n",
    "    if size != 7:  # wrong size :(\n",
    "        raise ValueError('data file is wrong size ({}).'.format(size))\n",
    "    return data_path\n",
    "\n",
    "def read_reuters_file(filename, vocab_freqs, token_func):\n",
    "    \"\"\"Reads in a whole file & finds the necessary bits\n",
    "    Returns a list of [text, label, set, id] where set is one of {train, test, valid}.\n",
    "    according to the \"ModHayes\" split. Adds to counts in vocab_freqs, tokenizing using the \n",
    "    token_func function.\n",
    "    \"\"\"\n",
    "    # the files have some things that make the xml parser unhappy\n",
    "    # there are probably efficient ways to do this, if so this is\n",
    "    # not one of them\n",
    "    with open(filename, errors='ignore') as raw_file:\n",
    "        file_str = raw_file.read()\n",
    "        file_str = re.sub('&#\\d{1,2};', '', file_str)\n",
    "        # the docs don't have any kind of root tags\n",
    "        # so we skip the doctype and wrap them in one\n",
    "        file_str = '<root>' + file_str[file_str.find('\\n'):-1] + '</root>'\n",
    "        print('..parsing {}'.format(filename))\n",
    "        root = ET.fromstring(file_str)\n",
    "        data = []\n",
    "        for child in root:\n",
    "            if child.attrib['TOPICS'] == 'YES':  # we need to be able to evaluate\n",
    "                try:\n",
    "                    text = child.find('./TEXT/BODY').text\n",
    "                except AttributeError:\n",
    "                    text = child.find('./TEXT').text  # should check type=brief\n",
    "                text = token_func(text)\n",
    "                for symbol in text:\n",
    "                    vocab_freqs[symbol] += 1\n",
    "                topics = [d.text for d in child.findall('./TOPICS/D')]\n",
    "                data.append(\n",
    "                    [text, topics, \n",
    "                    'train' if child.attrib['CGISPLIT'] == 'TRAINING-SET' else 'test',\n",
    "                    child.attrib['NEWID']])\n",
    "        return data\n",
    "    \n",
    "def word_split(text):\n",
    "    \"\"\"roughly tokenise into words. should do some stemming or something\"\"\"\n",
    "    # replace numbers with a special token\n",
    "    text = text.casefold()\n",
    "    text = re.sub('(\\d+([.,]?))+', ' <NUMBER> ', text)\n",
    "    text = re.sub('[.?!]', ' <STOP> ', text)\n",
    "    # remove remaining punctuation\n",
    "    text = re.sub(r'[^\\w\\s<>]', ' <PUNCT> ', text)\n",
    "    return text.split()\n",
    "\n",
    "def char_split(text):\n",
    "    \"\"\"just spit it out as a list of characters\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "def report_statistics(data):\n",
    "    \"\"\"data is a sequence of sequences\"\"\"\n",
    "    cumulative = 0\n",
    "    longest = -1\n",
    "    shortest = 1000000\n",
    "    for item in data:\n",
    "        length = len(item)\n",
    "        cumulative += length\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "        if length < shortest:\n",
    "            shortest = length\n",
    "    mean = cumulative/len(data)\n",
    "    cumulative = 0\n",
    "    for item in data:\n",
    "        cumulative += (mean - len(item))**2\n",
    "    stddev = math.sqrt(cumulative / len(data))\n",
    "    print('..mean: {} (stddev: {})'.format(mean, stddev))\n",
    "    print('..max: {}'.format(longest))\n",
    "    print('..min: {}'.format(shortest))\n",
    "\n",
    "def get_reuters(data_dir='data', level='word', min_reps=5):\n",
    "    \"\"\"gets the reuters dataset as (training, test, vocab).\n",
    "    training and test are sequences of sequences of ints (we'll tensor them up\n",
    "    when we iterate them later) and vocab is a list of strings.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        with tarfile.open(maybe_download(), 'r:gz') as datafile:\n",
    "            print('Extracting archive')\n",
    "            datafile.extractall(path=data_dir)\n",
    "    # the data is probably small enough that we will just hold it in memory\n",
    "    filenames = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if re.search('.sgm$', f)]\n",
    "    all_data = []\n",
    "    vocab_freqs = collections.Counter()\n",
    "    split_func = word_split if level == 'word' else char_split\n",
    "    for filename in filenames:\n",
    "        start = len(all_data)\n",
    "        all_data.extend(read_reuters_file(filename, vocab_freqs, split_func))\n",
    "        end = len(all_data)\n",
    "        #print('...{} new items'.format(end-start))\n",
    "    print('got {} in total'.format(len(all_data)))\n",
    "    print('vocab: {}'.format(len(vocab_freqs)))\n",
    "    print('Top 10: ')\n",
    "    ordered_words = vocab_freqs.most_common()\n",
    "    for word, count in ordered_words[0:10]:\n",
    "        print('  {} ({})'.format(word, count))\n",
    "    print('Bottom 10: ')\n",
    "    for word, count in ordered_words[-1:-10:-1]:\n",
    "        print('  {} ({})'.format(word, count))\n",
    "\n",
    "    print('removing least frequent (<= {} repetitions)'.format(min_reps))\n",
    "    to_remove = set()\n",
    "    for word, count in ordered_words[-1:0:-1]:\n",
    "        # loop through backwards\n",
    "        if count > min_reps:\n",
    "            break  # done\n",
    "        to_remove.add(word)\n",
    "        del vocab_freqs[word]\n",
    "    print('..{} symbols to remove'.format(len(to_remove)))\n",
    "    print('..new vocab size {}'.format(len(vocab_freqs)))\n",
    "    for item in all_data:\n",
    "        # ditch the ones we don't want\n",
    "        item[0] = ['<UNK>' if i in to_remove else i for i in item[0]]\n",
    "    \n",
    "    print('Converting to id sequences...')\n",
    "    # now we need to map the vocab to integers\n",
    "    # map the specials by hand\n",
    "    symbol_to_id = {'<GO>': 0, '<UNK>': 1, '<STOP>': 3, '<PUNCT>': 4, '<PAD>': 5}\n",
    "    id_num = 1  # save 0 for the GO symbol\n",
    "    for symbol in vocab_freqs:\n",
    "        symbol_to_id[symbol] = id_num\n",
    "        id_num += 1\n",
    "    for item in all_data:\n",
    "        item[0] = [symbol_to_id[i] for i in item[0]]\n",
    "    #for file in all_data[0:5]:\n",
    "     #   print(file)\n",
    "    # collect training sequences\n",
    "    training = [item[0] for item in all_data if item[2] == 'train']\n",
    "    \n",
    "    # let's have a look at some stats\n",
    "    report_statistics(training)\n",
    "    \n",
    "    return (training, None, symbol_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..parsing data/reut2-000.sgm\n",
      "..parsing data/reut2-001.sgm\n",
      "..parsing data/reut2-002.sgm\n",
      "..parsing data/reut2-003.sgm\n",
      "..parsing data/reut2-004.sgm\n",
      "..parsing data/reut2-005.sgm\n",
      "..parsing data/reut2-006.sgm\n",
      "..parsing data/reut2-007.sgm\n",
      "..parsing data/reut2-008.sgm\n",
      "..parsing data/reut2-009.sgm\n",
      "..parsing data/reut2-010.sgm\n",
      "..parsing data/reut2-011.sgm\n",
      "..parsing data/reut2-012.sgm\n",
      "..parsing data/reut2-013.sgm\n",
      "..parsing data/reut2-014.sgm\n",
      "..parsing data/reut2-015.sgm\n",
      "..parsing data/reut2-016.sgm\n",
      "..parsing data/reut2-017.sgm\n",
      "..parsing data/reut2-018.sgm\n",
      "..parsing data/reut2-019.sgm\n",
      "..parsing data/reut2-020.sgm\n",
      "..parsing data/reut2-021.sgm\n",
      "got 13476 in total\n",
      "vocab: 30639\n",
      "Top 10: \n",
      "  <PUNCT> (136070)\n",
      "  <NUMBER> (109129)\n",
      "  the (91914)\n",
      "  <STOP> (82619)\n",
      "  of (47890)\n",
      "  to (44521)\n",
      "  in (35331)\n",
      "  and (34636)\n",
      "  a (33162)\n",
      "  said (33096)\n",
      "Bottom 10: \n",
      "  reintensification (1)\n",
      "  betweenm (1)\n",
      "  robustas (1)\n",
      "  torino> (1)\n",
      "  elaboration (1)\n",
      "  disution (1)\n",
      "  overvaluing (1)\n",
      "  cos> (1)\n",
      "  perconal (1)\n",
      "removing least frequent (<= 50 repetitions)\n",
      "..27904 symbols to remove\n",
      "..new vocab size 2735\n",
      "Converting to id sequences...\n",
      "..mean: 142.33490931638505 (stddev: 161.8866134215189)\n",
      "..max: 2517\n",
      "..min: 2\n"
     ]
    }
   ],
   "source": [
    "training, _, vocab = get_reuters(level='word', min_reps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's see about making a seq2seq model to learn this biz\n",
    "batch_size = 128\n",
    "num_layers = 1\n",
    "num_units = 2\n",
    "def get_model(inputs, first=True):\n",
    "    # we should use buckets to efficiently handle variable length\n",
    "    # (it is highly variable)\n",
    "    GO = tf.constant([vocab['<GO>']]*batch_size, dtype=tf.int32)\n",
    "    loss_weights = tf.constant([1/batch_size] * batch_size)\n",
    "    with tf.variable_scope('rnn') as scope:\n",
    "        if not first:\n",
    "            scope.reuse_variables()\n",
    "        enc_inputs = [tf.placeholder(\n",
    "                        tf.int32, shape=[None],name=\"encoder{}\".format(i))\n",
    "                      for i in range(len(inputs))]\n",
    "        \n",
    "        outputs, state = tf.nn.seq2seq.embedding_tied_rnn_seq2seq(\n",
    "            enc_inputs, \n",
    "            [GO]*len(inputs),  # decoder inputs\n",
    "            tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.LSTMCell(\n",
    "                    num_units,\n",
    "                    num_units,  # number of inputs (embedding handles the rest)\n",
    "                    )]*num_layers),\n",
    "            len(vocab),\n",
    "            output_projection=None,  # probably should be some\n",
    "            dtype=tf.float32,\n",
    "            feed_previous=True)\n",
    "#         with tf.variable_scope('output_proj',\n",
    "#                                initializer=tf.random_normal_initializer()):\n",
    "#             softmax_w = tf.get_variable('W', [len(vocab), num_units])\n",
    "#             softmax_b = tf.get_variable('b', [len(vocab)])\n",
    "#             logits = [tf.matmul(softmax_w, output, transpose_b=True) + b for output in outputs]\n",
    "        # TODO(pfcm): calculate weights depending on padding\n",
    "        loss = tf.nn.seq2seq.sequence_loss(outputs,\n",
    "                                           inputs, # goal is to reproduce\n",
    "                                           [loss_weights]*len(inputs))\n",
    "        # now optimise\n",
    "        lr = tf.get_variable('learning_rate', [1], tf.float32, trainable=False)\n",
    "        mo = tf.get_variable('momentum', [1], tf.float32, trainable=False)\n",
    "        gn = tf.get_variable('grad_norm', [1], tf.float32, trainable=False)\n",
    "        optimiser = tf.train.MomentumOptimizer(lr, mo)\n",
    "        params = tf.trainable_variables()\n",
    "        grads = tf.gradients(loss, params)\n",
    "        clipped_grads, norm = tf.clip_by_global_norm(grads, gn)\n",
    "        train_op = optimiser.apply_gradients(\n",
    "            zip(clipped_grads, params))\n",
    "        save = tf.train.Saver(tf.all_variables())\n",
    "        return enc_inputs, loss, train_op, (lr, mo, gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def batch_iterator(data, batch_size):\n",
    "    \"\"\"generator to iterate batches\"\"\"\n",
    "    idces = np.arange(len(data))\n",
    "    np.random.shuffle(idces)\n",
    "    def partition(itr, n):\n",
    "        r = iter(itr)\n",
    "        res = None\n",
    "        while True:\n",
    "            res = list(itertools.islice(itr, 0, n))\n",
    "            if res == []:\n",
    "                break\n",
    "            yield res\n",
    "    \n",
    "    # iterate batch_size chunks of idces\n",
    "    for idx_chunk in partition(idces, batch_size):\n",
    "        # grab the sequences and batch em up\n",
    "        batch = []\n",
    "        batch_list = [data[i] for i in idx_chunk]\n",
    "        # what is the longest item in the batch?\n",
    "        longest = max((len(item) for item in batch_list))\n",
    "        for i in range(longest):\n",
    "            batch.append(np.array(\n",
    "                [item[i] if i < len(item) else vocab['<PAD>'] for item in batch_list]))\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear the graph for safe re-running of this cell\n",
    "tf.reset_default_graph()\n",
    "# start a session\n",
    "learning_rate = 0.1\n",
    "momentum = 0.85\n",
    "gradnorm = 5\n",
    "with tf.Session() as sess:\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in batch_iterator(training, batch_size):\n",
    "            # unrolls per batch, probably silly\n",
    "            # although we will see once it actually does a couple of batches\n",
    "            in_vars, loss_op, train_op, hps = get_model(batch,\n",
    "                                                   epoch==0 and num_batches==0)\n",
    "            feed = {zip(in_vars, batch)}\n",
    "            feed[hps[0]] = learning_rate\n",
    "            feed[hps[1]] = momentum\n",
    "            feed[hps[2]] = gradnorm\n",
    "            batch_loss, _ = sess.run([loss_op, train_op],\n",
    "                                     {zip(in_vars, batch)})\n",
    "            loss += batch_loss\n",
    "            num_batches += 1\n",
    "            print('.', end='', flush=True)\n",
    "        print('\\nEpoch {}, loss {}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
